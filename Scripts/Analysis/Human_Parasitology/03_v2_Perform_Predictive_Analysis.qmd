---
title: "Parasitological Analysis"
format:
  html:
    self-contained: true
    toc: false
    code-fold: true
    code-summary: "Show the code"
    theme: default
    output-file: "Parasitological_Analysisv5.html" #MAKE SURE TO UPDATE
css: styles.css
execute:
  echo: true
---


## Analysis Overview
##### Data source: NSF-Senegal project, 
 - Baseline & Midline CRDES survey data
 - Baseline & Midline EPLS & UCAD parasitological data
 - Baseline & Midline Notre Dame Ecological Data


This analysis proceeds in **two steps** for each of the primary outcome variables of interest:

1. **Lasso-Logit Regression**  
   We first use Lasso regression to identify the most predictive covariates for each outcome. This helps reduce dimensionality and guard against overfitting.

2. **Evaluation using Confusion Matrices**  
   We apply the fitted model to the test data and generate confusion matrices to assess predictive performance, including metrics such as sensitivity, specificity, and overall accuracy.

---

## Outcome Variables

We focus on three infection indicators, constructed from parasitological data:
<!-- 
-  **`sh_inf`: S. haematobium infection indicator**  
  Equals 1 if egg count from either urine filtration test (`fu_p1` or `fu_p2`) is greater than 0; otherwise 0.  
  *Represents urinary schistosomiasis.*

- **`sm_inf`: S. mansoni infection indicator**  
  Equals 1 if egg count from any of the four stool slides (`p1_kato1_k1_pg`, `p1_kato2_k2_peg`, `p2_kato1_k1_epg`, `p2_kato2_k2_epg`) is greater than 0; otherwise 0.  
  *Represents intestinal schistosomiasis.* -->

- **`sm_sh_inf`: Joint-Infection indicator**  
  Equals 1 if either `sh_inf` or `sm_inf` is equal to 1.  
  *Captures cases of either or both infections.*
  

<!-- INITIATE HERE - load packages -->


```{r, echo=FALSE, message=FALSE, warning=FALSE}

# packages <- c("clusterSEs", "mlogit", "writexl", "dplyr", "readr", "tidyr",
#               "haven", "data.table", "tidyverse", "estimatr", "broom", "kableExtra", "haven", "caret",
#               "sandwich", "lmtest", "stats", "nnet", "car", "aod", "clubSandwich", "glmnet", "multiwayvcov")

packages <- c("dplyr", "readr", "tidyr", "stargazer",
             "tidyverse", "kableExtra", "haven", "detectseparation", "kableExtra",
              "sandwich", "lmtest", "stats", "clubSandwich", "glmnet", "multiwayvcov", "hdm", "fs", "xfun", "quarto", "labelled", "patchwork")

# install.packages("haven")
# library("haven")
# Install any missing packages
new_packages <- packages[!(packages %in% installed.packages()[, "Package"])]
if (length(new_packages)) install.packages(new_packages, dependencies = TRUE)

# Load the required packages without printing any output
invisible(lapply(packages, library, character.only = TRUE))


```


<!-- Define project specific paths -->

```{r, echo=FALSE, message=FALSE, warning=FALSE}
proj_paths <- list(
  projects = "C:/Users/Kateri/Box/NSF Senegal",
  alternative_projects = "C:/Users/km978/Box/NSF Senegal"
)

# Choose the correct project root based on availability
proj_root <- if (file.exists(proj_paths$projects)) {
  proj_paths$projects
} else {
  proj_paths$alternative_projects
}

# Define project subpaths manually using file.path for cross-platform safety
proj <- list(
  p1 = file.path(proj_root, "Data_Management", "Output", "Analysis", "Human_Parasitology", "Analysis_Data")
)

# Full path to Stata file
file_path_infection_df <- file.path(proj$p1, "03_mid_base_analysis_df.dta")

# Read the dataset
infection_df <- read_dta(file_path_infection_df)

```



```{r, echo=FALSE, message=FALSE, warning=FALSE}

analysis_df <- infection_df %>%
  select(-p2_avg, -q_51, -asset_index, -hh_10_, -fishing) %>%
  na.omit()

analysis_df <- analysis_df %>%
  mutate(age_diff = hh_age_ - age_hp) %>%
  filter(abs(age_diff) < 4)

analysis_df <- analysis_df %>%
  select(-age_diff)


# nrow(analysis_df)
# nrow(infection_df)

#view(infection_df)

```

::: {.panel-tabset}

### Training testing data

Here we create Training and Testing data to perform prediction by subsetting 70% of the data as training observations, and 30% as testing. 

```{r, message=FALSE, warning=FALSE}

# reproducibility 
set.seed(20240611)

# make matrix sith total number of observations
n <- nrow(analysis_df)

# create a vector of shuffled row indices
shuffled_indices <- sample(n)

# compute the number of training observations (70%)
train_size <- floor(0.7 * n)

# indices for training and testing
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):n]

# now subset the dataframes
train_df <- analysis_df[train_indices, ]
test_df <- analysis_df[test_indices, ]


# check the sizes
cat("Training set size:", nrow(train_df), "\n")
cat("Testing set size:", nrow(test_df), "\n")

```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

## reorder summary data

sum_data <- analysis_df %>%
  relocate(sh_inf:total_egg, .before = living_01_bin) %>%
  relocate(age_hp, .before = hh_gender_) %>%
  relocate(sex_hp, .before = hh_10_w99)


```


### Summary Stats



```{r, message=FALSE, warning=FALSE}
      


summary_stats <- function(data, exclude = NULL) {
  # Remove excluded columns
  if (!is.null(exclude)) {
    data <- data[ , !(names(data) %in% exclude)]
  }

  # Keep only numeric columns
  data_numeric <- data[ , sapply(data, is.numeric)]

  # Remove columns where all values are NA
  data_numeric <- data_numeric[ , colSums(!is.na(data_numeric)) > 0, drop = FALSE]

  # Extract variable labels (if present)
  labels <- sapply(data_numeric, function(x) attr(x, "label"))

 # Compute summary statistics
  summary_df <- data.frame(
    Question = labels,
    Variable = names(data_numeric),
    N = sapply(data_numeric, function(x) sum(!is.na(x))),
    Min = round(sapply(data_numeric, function(x) min(x, na.rm = TRUE)), 2),
    Mean = round(sapply(data_numeric, function(x) mean(x, na.rm = TRUE)), 2),
    Max = round(sapply(data_numeric, function(x) max(x, na.rm = TRUE)), 2),
    SD = round(sapply(data_numeric, function(x) sd(x, na.rm = TRUE)), 2),
    row.names = NULL,
    stringsAsFactors = FALSE
  )

  return(summary_df)
}
```


Summary stats for the analysis data. I imputed per PAP, dropped p2_avg and rows with missings, and used winsorized/standardized versions of q_51, asset_index, and hh_10_.


```{r, echo=FALSE, message=FALSE, warning=FALSE}

 paras_sum_stats_ad <- summary_stats(sum_data, exclude = c("hhid_village", "village_name", "hhid", "individ", "identificant","match_score", "round" ))

```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

## Check # observations between baseline and midline 

# analysis_df %>% filter(round == 1) %>% nrow()
# analysis_df %>% filter(round == 2) %>% nrow()

```

:::{.scrolling}

```{r}
# add section and source labels based on variable ranges
## this is done dynamically so I don't have to replace the header or Source variable if we add new variables to the section.

paras_sum_stats_ad <- paras_sum_stats_ad %>%
  mutate(
    section = case_when(
      Variable %in% Variable[match("hh_age_", Variable):match("total_egg", Variable)] ~ "Individual-level Variables",
      Variable %in% Variable[match("living_01_bin", Variable):match("asset_index_std", Variable)] ~ "Household-level Variables",
      Variable %in% Variable[match("q_51_w99", Variable):match("InfectedBiomphalaria", Variable)] ~ "Community-level Variables",
      TRUE ~ "Other"
    ),
    Source = case_when(
      Variable %in% c("age_hp", "sex_hp") ~ "EPLS/UCAD Parasitological Data",  # <- Specific override
      Variable %in% Variable[match("hh_age_", Variable):match("health_5_9_", Variable)] ~ "CRDES Household Survey",
        Variable %in% Variable[match("living_01_bin", Variable):match("asset_index_std", Variable)] ~ "CRDES Household Survey",
      Variable == "q_51" | Variable == "q_51_w99" ~ "CRDES Community Survey",
      Variable %in% Variable[match("sh_inf", Variable):match("total_egg", Variable)] ~ "EPLS/UCAD Parasitological Data",
      Variable %in% Variable[match("schisto_indicator", Variable):match("InfectedBiomphalaria", Variable)] ~ "Notre Dame Ecological Data",
      TRUE ~ NA_character_
    ),
    row_id = row_number()
  )


# Prepare the base table
kbl_base <- paras_sum_stats_ad %>%
  select(-section, -row_id) %>%
  kbl(format = "html", caption = "Summary Statistics for Analysis Data, Baseline & Midline")

# Add group_rows dynamically by section
for (sec in unique(paras_sum_stats_ad$section)) {
  rows <- paras_sum_stats_ad %>% filter(section == sec) %>% pull(row_id)
  kbl_base <- kbl_base %>% group_rows(sec, min(rows), max(rows))
}

# Add styling, scrolling, and footnote
kbl_base %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE,
    position = "left"
  ) %>%
  scroll_box(width = "100%", height = "600px") %>%  # enables scrolling
  footnote(general = "Note: Baseline: 183 observations, Midline: 161 observations. Means are calculated from non-missing values.")


```

:::



### sm_sh_inf Analysis

::: {.panel-tabset}

#### Model specification 


This code runs a Lasso logistic regression with cross-validation to select key predictors of sm_sh_inf. It identifies the optimal lambda, fits the model, and extracts the non-zero (important) variables.


```{r}

exclude_vars <- c("hhid_village", "village_name", "hhid", "individ",
                  "identificant", "match_score", "round", "sh_inf", "sm_sh_inf", "sm_inf", "sh_egg_count","p1_avg", "sm_egg_count", "total_egg")  # Exclude outcome too

# Grab all other column names
  covariates <- setdiff(names(train_df), exclude_vars)

  # Create predictor matrix
  x <- data.matrix(train_df[, covariates])
  y <- train_df$sm_sh_inf

#perform k-fold cross-validation to find optimal lambda value
  k_val <- cv.glmnet(x, y, family = "binomial", alpha = 1)

    best_lambda <- k_val$lambda.min

#produce plot of test MSE by lambda value
    plot(k_val)

#find coefficients of best model
model <- glmnet(x, y, alpha = 1, lambda = best_lambda, family = "binomial")

#model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
  coef(model)

#print regression table
  print(model)

# Extract coefficients at best lambda
  mat <- coef(model, s = best_lambda)

# Convert to named vector and find non-zero coefficients
  vars_raw <- rownames(mat)[mat[, 1] != 0]

# Remove intercept from selected vars
  vars <- setdiff(vars_raw, c("(Intercept)")) #confirm the precise variables we're interested in

  # kable(vars, format = "html", caption = "Best Predictors for Joint-Infection") %>%
  #     kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left") %>%
  #     scroll_box(width = "100%")

  
```




```{r}
# display best vars with labels 

vars_raw <- rownames(mat)[mat[, 1] != 0]
vars <- setdiff(vars_raw, "(Intercept)")

# create a dataframe with variable names and their corresponding labels from train_df

var_labels <- tibble(
  variable = vars,
  label = sapply(vars, function(v) {
    if (v %in% names(train_df)) {
      var_label(train_df[[v]])
    } else {
      NA  # in case it's an interaction or transformed variable
    }
  })
)

var_labels <- var_labels %>% 
  relocate(label, .before = variable)

# display with kable 


kable(var_labels, format = "html", caption = "Best Predictors for Joint-Infection") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left") %>%
  scroll_box(width = "100%")


```



#### Prediction - all

<!--
We will:

- **Compute predicted probabilities** for each individual using the model.
- **Threshold Selection**: Start with a 0.5 threshold for classifying individuals as infected (probability > 0.5) or uninfected (probability â‰¤ 0.5).
- **Confusion Matrix**: Generate a 2x2 confusion matrix comparing predicted versus actual infections (infected vs. non-infected).
  - **False Negative Rate (FNR)**: The proportion of actual infected individuals incorrectly predicted as uninfected.
  - **False Positive Rate (FPR)**: The proportion of uninfected individuals incorrectly predicted as infected.
- **Threshold Iteration**: Incrementally adjust the threshold (e.g., from 0.1 to 0.9) and recalculate the confusion matrix, FNR, and FPR at each step.
- **Optimization**: Identify the threshold that minimizes the combined classification error (FNR + FPR).
-->

The following output are the FPRs and FNRs using the logit-lasso coefficients. These are summarized in one confusion matrix. 

```{r, message=FALSE, warning=FALSE}

#get test data
x_test <- data.matrix(test_df[, covariates])  # only predictors

test_probs <- predict(model, newx = as.matrix(x_test), type = "response")

### Generalized to function 

evaluate_thresholds <- function(test_probs, actual, thresholds = seq(0.1, 0.9, by = 0.1)) {
  results <- data.frame()
  
  for (threshold in thresholds) {
    pred_class <- ifelse(test_probs > threshold, 1, 0)
    
    TP <- sum(pred_class == 1 & actual == 1)
    TN <- sum(pred_class == 0 & actual == 0)
    FP <- sum(pred_class == 1 & actual == 0)
    FN <- sum(pred_class == 0 & actual == 1)
    
    FPR <- ifelse((FP + TN) > 0, FP / (FP + TN), NA)
    FNR <- ifelse((FN + TP) > 0, FN / (FN + TP), NA)
    Sum_FPR_FNR <- FPR + FNR
    
    results <- rbind(results, data.frame(
      Threshold = threshold,
      TP = TP, TN = TN, FP = FP, FN = FN,
      FPR = round(FPR, 3),
      FNR = round(FNR, 3),
      Sum_FPR_FNR = round(Sum_FPR_FNR, 3)
    ))
  }
  
  # Find threshold that minimizes FPR + FNR
  min_row <- results[which.min(results$Sum_FPR_FNR), ]
  attr(results, "best_threshold") <- min_row$Threshold
  
  return(results)
}
 
  results_df <- evaluate_thresholds(test_probs, test_df$sm_sh_inf)
#print(results_df)

# Best threshold (minimizing FPR + FNR)
  best_threshold <- attr(results_df, "best_threshold")

kable(results_df, format = "html", caption = "Confusion Matrix for sm_sh_inf at each threshold") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left") %>%
  scroll_box(width = "100%")

kable(best_threshold, format = "html", caption = "Best Threshold") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left") %>%
  scroll_box(width = "100%")

```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

##Output table as latex document 

# Create LaTeX table string
confusion_latex <- kable(results_df, format = "latex", booktabs = TRUE, 
                         caption = "Confusion Matrix for sm\\_sh\\_inf at each threshold") %>%
  kable_styling(latex_options = c("hold_position", "striped"))

# Write to .tex file
writeLines(confusion_latex, "C:/Users/Kateri/Downloads/NSF-Senegal/Tables_Figures/Parasitological_Predictive_Analysis/Confusion_matrix.tex")

# Repeat for best threshold
threshold_latex <- kable(best_threshold, format = "latex", booktabs = TRUE, 
                         caption = "Best Threshold") %>%
  kable_styling(latex_options = c("hold_position", "striped"))

writeLines(threshold_latex, "C:/Users/Kateri/Downloads/NSF-Senegal/Tables_Figures/Parasitological_Predictive_Analysis/best_threshold.tex")

```


##### Summarize and visualize the probababilities

This is a scatter plot of the predicted probability of infection against the egg count. Any dark red observations are the overlap of S. mansoni and S. haematobium. 

```{r, message=FALSE, warning=FALSE}
### visulalize 

# make a single df with that combines predictions and egg counts 
plot_df <- data.frame(
  predicted_prob = test_probs,
  sm_egg_count = test_df$sm_egg_count,
  sh_egg_count = test_df$sh_egg_count
)

plot_df <- plot_df %>%
  rename(predicted_prob = s0)


# create scatter plot with color-coded species

 ggplot(plot_df) +
  geom_point(aes(x = predicted_prob, y = sm_egg_count, color = "S. mansoni"), alpha = 0.7) +
  geom_point(aes(x = predicted_prob, y = sh_egg_count, color = "S. haematobium"), alpha = 0.7) +
  scale_color_manual(values = c("S. mansoni" = "blue", "S. haematobium" = "red")) +
  labs(
    title = "Predicted Probability vs Egg Counts",
    x = "Predicted Probability of Infection",
    y = "Egg Count",
    color = "Species"
  ) +
  theme_minimal()


```

```{r, message=FALSE, warning=FALSE}

### visulalize 

test_probs_df <- summary(test_probs)

kable(test_probs_df, format = "html", caption = "Summary of Probabilities") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left") %>%
  scroll_box(width = "100%")


#hist(test_probs, breaks = 20, col = "skyblue", main = "Distribution of Predicted Probabilities")

hist(test_probs,
     breaks = 20,
     col = "skyblue",
     main = "Distribution of Predicted Probabilities",
     xlab = "Predicted Probability",
     xlim = c(0, 1))


```




:::










